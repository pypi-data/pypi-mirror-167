# -*- coding: utf-8 -*-
from setuptools import setup

package_dir = \
{'': 'src'}

packages = \
['hiraishin', 'hiraishin.models', 'hiraishin.schema']

package_data = \
{'': ['*']}

install_requires = \
['click>=8.0.0,<9.0.0',
 'hydra-core==1.1.1',
 'overrides>=3.1.0,<4.0.0',
 'pydantic>=1.8.1,<2.0.0',
 'pytorch-lightning>=1.6.0,<2.0.0']

entry_points = \
{'console_scripts': ['hiraishin = hiraishin.cli:cmd']}

setup_kwargs = {
    'name': 'hiraishin',
    'version': '0.2.1',
    'description': 'A thin PyTorch-Lightning wrapper for building configuration-based DL pipelines with Hydra.',
    'long_description': "# Hiraishin\nA thin PyTorch-Lightning wrapper for building configuration-based DL pipelines with Hydra.\n\n# Dependencies\n- PyTorch Lightning\n- Hydra\n- Pydantic\n- etc.\n\n# Installation\n\n```shell\n$ pip install -U hiraishin\n```\n\n# Basic workflow\n## 1. Model initialization with type annotations\nDefine a model class that has training components with type annotations.\n\n```python\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom hiraishin.models import BaseModel\n\n\nclass ToyModel(BaseModel):\n\n    net: nn.Linear\n    criterion: nn.CrossEntropyLoss\n    optimizer: optim.Adam\n    scheduler: optim.lr_schedulers.ExponentialLR\n\n    def __init__(self, config: DictConfig) -> None:\n        super().__init__(config)\n```\n\nModules with the following prefixes are instantiated by their own role-specific logic.\n\n- `net`\n- `criterion`\n- `optimizer`\n- `scheduler`\n\nThe same notation can be used to define components other than the learning components listed above (e.g., tokenizers). It is also possible to define built-in type constants that are YAML serializable.\n\n```python\nclass ToyModel(BaseModel):\n\n    net: nn.Linear\n    criterion: nn.CrossEntropyLoss\n    optimizer: optim.Adam\n    scheduler: optim.lr_schedulers.ExponentialLR\n\n    # additional components and constants\n    tokenizer: MyTokenizer\n    n_classes: int\n\n    def __init__(self, config: DictConfig) -> None:\n        super().__init__(config)\n```\n\n## 2. Configuration file generation\nHiraishin provides a CLI command that automatically generates a configuration file based on type annotations.\n\nFor example, if `ToyModel` is defined in `models.py` (i.e., `from models import ToyModel` can be executed in the code), then the following command will generate the configuration file automatically.\n\n```shell\n$ hiraishin generate model.ToyModel --output_dir config/model\nThe config has been generated! --> config/model/ToyModel.yaml\n```\n\nLet's take a look at the generated file.\n\n```yaml\n_target_: models.ToyModel\n_recursive_: false\nconfig:\n\n  networks:\n    net:\n      args:\n        _target_: torch.nn.Linear\n        out_features: ???\n        in_features: ???\n      weights:\n        initializer: null\n        path: null\n\n  losses:\n    criterion:\n      args:\n        _target_: torch.nn.CrossEntropyLoss\n      weight: 1.0\n\n  optimizers:\n    optimizer:\n      args:\n        _target_: torch.optim.Adam\n      params:\n      - ???\n      scheduler:\n        args:\n          _target_: torch.optim.lr_scheduler.ExponentialLR\n          gamma: ???\n        interval: epoch\n        frequency: 1\n        strict: true\n        monitor: null\n\n  tokenizer:\n    _target_: MyTokenizer\n  n_classes: ???\n\n```\n\nFirst of all, it is compliant with the instantiation by `hydra.utils.instantiate`.\n\nThe positional arguments are filled with `???` that indicates mandatory parameters. They should be overridden by the values you want to set.\n\n## 3. Training routines definition\nThe rest of model definition is only defining your training routine along with the style of PyTorch Lightning.\n\n```python\nclass ToyModel(BaseModel):\n    \n    ...\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.net(x)\n\n    def training_step(self, batch, *args, **kwargs) -> torch.Tensor:\n        x, target = batch\n        pred = self.forward(x)\n        loss = self.criterion(pred, target)\n        self.log('loss/train', loss)\n        return loss\n```\n\n## 4. Model Instantiation\nThe defined model can be instantiated from configuration file. Let's train your models!\n\n```python\nfrom hydra.utils import inatantiate\nfrom omegeconf import OmegaConf\n\n\ndef app():\n    ...\n\n    config = OmegaConf.load('config/model/toy.yaml')\n    model = inatantiate(config)\n\n    print(model)\n    # ToyModel(\n    #     (net): Linear(in_features=1, out_features=1, bias=True)\n    #     (criterion): CrossEntropyLoss()\n    # )\n\n    trainer.fit(model, ...)\n```\n\n## 5. Model loading\nYou can easily load trained models by using the checkpoints generated by PyTorch Lightning's standard features. Let's test your models!\n\n```python\nfrom hiraishin.utils import load_from_checkpoint\n\nmodel = load_from_checkpoint('path/to/model.ckpt')\nprint(model)\n# ToyModel(\n#     (net): Linear(in_features=1, out_features=1, bias=True)\n#     (criterion): CrossEntropyLoss()\n# )\n```\n\n# License\nHiraishin is licensed under the Apache License, Version 2.0. See [LICENSE](LICENSE) for the full license text.\n",
    'author': 'So Uchida',
    'author_email': 's.aiueo32@gmail.com',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'None',
    'package_dir': package_dir,
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'entry_points': entry_points,
    'python_requires': '>=3.8,<4.0',
}


setup(**setup_kwargs)
