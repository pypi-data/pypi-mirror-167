# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/10_training.plsr.ipynb.

# %% auto 0
__all__ = ['Learner', 'PLS_model', 'Evaluator']

# %% ../nbs/10_training.plsr.ipynb 3
#nbdev_comment from __future__ import annotations

# Python utils
from collections import OrderedDict
from tqdm.auto import tqdm

# mirzai utils
from ..data.loading import load_kssl
from ..data.selection import (select_y, select_tax_order, select_X)
from ..data.transform import (log_transform_y, SNV, TakeDerivative,
                                   DropSpectralRegions, CO2_REGION)
from .metrics import eval_reg
from .core import is_plateau

# Data science stack
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.cross_decomposition import PLSRegression
from sklearn.metrics import mean_squared_error

from fastcore.test import *
from fastcore.transform import compose

# %% ../nbs/10_training.plsr.ipynb 9
class Learner():
    def __init__(self, data, model):
        self.X_train, self.X_valid, self.y_train, self.y_valid = data
        self.model = model
        self.n_cpts_best = 1
        
    def find_hp(self, n_cpts_range=range(1,10), delta=0.005, verbose=True):
        perfs = OrderedDict({'n_cpts': [], 'train': [], 'valid': []})
        
        for n_cpts in n_cpts_range:
            self.model.set_params(model__n_components=n_cpts)
            self.model.fit(self.X_train, self.y_train)
            perfs['n_cpts'].append(n_cpts)
            perfs['train'].append(self.model.score(self.X_train, self.y_train))
            perfs['valid'].append(self.model.score(self.X_valid, self.y_valid))
            self.n_cpts_best = n_cpts
            if (is_plateau(-np.array(perfs['valid']), delta=delta, verbose=verbose)):
                return perfs
        return perfs
    
    def fit(self, n_cpts=None):
        if not n_cpts:
            n_cpts = self.n_cpts_best
        self.model.set_params(model__n_components=n_cpts)
        self.model.fit(self.X_train, self.y_train)
        
    def evaluate(self, X, y):
        return self.model.score(X, y)

# %% ../nbs/10_training.plsr.ipynb 10
class PLS_model():
    "Partial Least Squares model runner"
    def __init__(self, X_names, pipeline_kwargs={}):
        self.X_names = X_names
        self.pipeline_kwargs = pipeline_kwargs
        self.model = None

    def fit(self, data):
        X, y = data
        self.model = Pipeline([
            ('snv', SNV()),
            ('derivative', TakeDerivative(**self.pipeline_kwargs['derivative'])),
            ('dropper', DropSpectralRegions(self.X_names, **self.pipeline_kwargs['dropper'])),
            ('model', PLSRegression(**self.pipeline_kwargs['model']))])
        self.model.fit(X, y)
        return self

    def predict(self, data):
        X, y = data
        return (self.model.predict(X), y)

    def eval(self, data, is_log=True):
        X, y = data
        return eval_reg(y, self.model.predict(X))

# %% ../nbs/10_training.plsr.ipynb 11
class Evaluator():
    def __init__(self, data, depth_order, X_names,
                 seeds=range(20), pipeline_kwargs={}, split_ratio=0.1):
        self.seeds = seeds
        self.X, self.y = data
        self.X_names = X_names
        self.depth_order = depth_order
        self.split_ratio = split_ratio
        self.pipeline_kwargs = pipeline_kwargs
        self.models = []
        self.perfs = OrderedDict({'train': [], 'test': []})

    def train_multiple(self):
        for seed in tqdm(self.seeds):
            X_train, X_test, y_train, y_test, depth_order_train, depth_order_test = self._splitter(seed)
            model = PLS_model(self.X_names, self.pipeline_kwargs)
            model.fit((X_train, y_train))
            self.models.append(model)

    def eval_on_train(self, reducer):
        perfs = []
        for i, seed in enumerate(self.seeds):
            X_train, X_test, y_train, y_test, _, _ = self._splitter(seed)
            perf = self.models[i].eval((X_train, y_train))
            perf['n'] = len(X_train)
            perfs.append(perf)
        if reducer:
            perfs = self.reduce(perfs, reducer)
        return perfs

    def eval_on_test(self, order=-1, reducer=None):
        perfs = []
        for i, seed in tqdm(enumerate(self.seeds)):
            X_train, X_test, y_train, y_test, depth_order_train, depth_order_test = self._splitter(seed)
            if order != - 1:
                mask = depth_order_test[:, 1] == order
                X_test, y_test = X_test[mask, :], y_test[mask]
            perf = self.models[i].eval((X_test, y_test))
            perf['n'] = len(X_test)
            perfs.append(perf)
        if reducer:
            perfs = self.reduce(perfs, reducer)
        return perfs

    def _splitter(self, seed):
        return train_test_split(self.X, self.y, self.depth_order,
                                test_size=self.split_ratio,
                                random_state=seed)

    def reduce(self, perfs, fn=np.mean):
        results = {}
        for metric in perfs[0].keys():
            result = fn(np.array([perf[metric] for perf in perfs]))
            results[metric] = result
        return results
