# This file was generated by mettle.genes.braze.GenPy3 [ver 2.2] on Fri Sep 16 12:08:12 2022
#

import datetime
import uuid
import mettle.braze
import mettle.lib

import traceback

from mettle.braze.iserver_interface import IServerInterface
from mettle.braze.iserver_marshaler import IServerMarshaler
from mettle.io.istream import IStream
from mettle.braze.server import Server
from mettle.lib.xmettle import xMettle
from .monitor_server_interface import MonitorServerInterface

from bs_monitor.db.tables import tBatch
from bs_monitor.db.tables import oBatchWithLatestInst
from bs_monitor.db.tables import tBatchInst
from bs_monitor.db.tables import tBatchItem
from bs_monitor.db.tables import tJob
from bs_monitor.db.tables import tJobInst
from bs_monitor.db.tables import oJobInstSearch
from bs_monitor.db.tables import tJobInstMetric
from bs_monitor.db.tables import tMondate

from .blazy_load import bLazyLoad
from .bbatch_query import bBatchQuery
from .bbatch_inst_query import bBatchInstQuery
from .bjob_inst_query import bJobInstQuery

class MonitorServerMarshaler(IServerMarshaler, MonitorServerInterface):
    __slots__ = ("_server", "_simpl")

    def __init__(self, server_impl: IServerInterface):
        """
        Constructor.

        :param server_impl: braze client object.
        """
        self._server   = None
        self._simpl    = server_impl

    def _signature(self) -> str:
        """
        Gets the server signature.

        :return: the signature of this server.
        """
        return 'Monitor'

    def _server_impl(self) -> IServerInterface:
        """
        Gets the server implementation.

        :return: the overloaded server interface/server implementation object.
        """
        return self._simpl

    def _serve(self, server: Server, remote_sig: "str|int", in_stream: IStream, impl_data: dict = None):
        """
        The main server entry point to the marshaler.

        :param server: The server object to use.
        :param remote_sig: The client rpc signature.
        :param in_stream:  The input stream.
        :param impl_data: Any optional impl server data.
        :return: a tuple of (out_stream, err_code, err_msg)
        """
        self._server = server
        out_stream   = None

        try:
            self._simpl._set_impl_data(impl_data)

            if remote_sig == 'batchCreate':
                out_stream = self.batch_create(in_stream)
            elif remote_sig == 'batchRead':
                out_stream = self.batch_read(in_stream)
            elif remote_sig == 'batchUpdate':
                out_stream = self.batch_update(in_stream)
            elif remote_sig == 'batchDelete':
                out_stream = self.batch_delete(in_stream)
            elif remote_sig == 'batchList':
                out_stream = self.batch_list(in_stream)
            elif remote_sig == 'batchLastestInstList':
                out_stream = self.batch_lastest_inst_list(in_stream)
            elif remote_sig == 'batchinstRead':
                out_stream = self.batchinst_read(in_stream)
            elif remote_sig == 'batchinstList':
                out_stream = self.batchinst_list(in_stream)
            elif remote_sig == 'batchitemCreate':
                out_stream = self.batchitem_create(in_stream)
            elif remote_sig == 'batchitemRead':
                out_stream = self.batchitem_read(in_stream)
            elif remote_sig == 'batchitemUpdate':
                out_stream = self.batchitem_update(in_stream)
            elif remote_sig == 'batchitemShift':
                out_stream = self.batchitem_shift(in_stream)
            elif remote_sig == 'batchitemDelete':
                out_stream = self.batchitem_delete(in_stream)
            elif remote_sig == 'batchitemList':
                out_stream = self.batchitem_list(in_stream)
            elif remote_sig == 'jobCreate':
                out_stream = self.job_create(in_stream)
            elif remote_sig == 'jobRead':
                out_stream = self.job_read(in_stream)
            elif remote_sig == 'jobUpdate':
                out_stream = self.job_update(in_stream)
            elif remote_sig == 'jobDelete':
                out_stream = self.job_delete(in_stream)
            elif remote_sig == 'jobList':
                out_stream = self.job_list(in_stream)
            elif remote_sig == 'jobSchedule':
                out_stream = self.job_schedule(in_stream)
            elif remote_sig == 'jobinstRead':
                out_stream = self.jobinst_read(in_stream)
            elif remote_sig == 'jobinstRerun':
                out_stream = self.jobinst_rerun(in_stream)
            elif remote_sig == 'jobinstForceOk':
                out_stream = self.jobinst_force_ok(in_stream)
            elif remote_sig == 'jobinstStop':
                out_stream = self.jobinst_stop(in_stream)
            elif remote_sig == 'jobinstList':
                out_stream = self.jobinst_list(in_stream)
            elif remote_sig == 'jobinstMetrics':
                out_stream = self.jobinst_metrics(in_stream)
            elif remote_sig == 'mondateCreate':
                out_stream = self.mondate_create(in_stream)
            elif remote_sig == 'mondateRead':
                out_stream = self.mondate_read(in_stream)
            elif remote_sig == 'mondateUpdate':
                out_stream = self.mondate_update(in_stream)
            elif remote_sig == 'mondateDelete':
                out_stream = self.mondate_delete(in_stream)
            elif remote_sig == 'mondateList':
                out_stream = self.mondate_list(in_stream)
            elif remote_sig == 'mondateIncrement':
                out_stream = self.mondate_increment(in_stream)
            elif remote_sig == 'mondateSetValue':
                out_stream = self.mondate_set_value(in_stream)
            elif remote_sig == 'purgeHistory':
                out_stream = self.purge_history(in_stream)
            elif remote_sig == 'serverDateTime':
                out_stream = self.server_date_time(in_stream)
            else:
                raise xMettle("Remote signature not known [%s]" % remote_sig)
        except xMettle as x:
            self._server.logger().error("Exception caught [client_address:%s, msg:%s, trace:%s]" % (self._server.get_transport().client_address(), str(x), traceback.format_exc()))

            if x.get_error_code() == xMettle.eCode.TerminalException:
                raise x

            return out_stream, x.get_error_code(), str(x)

        except Exception as x:
            self._server.logger().error("Exception caught [client_address:%s, msg:%s, trace:%s]" % (self._server.get_transport().client_address(), str(x), traceback.format_exc()))

            return out_stream, xMettle.eCode.UnknownException, str(x)

        return out_stream, 0, None

    def batch_create(self, _i):
        self._server.logger().debug("[batch_create - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        rec = tBatch()

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("batchCreate_IN")
        rec._deserialize(_r, "rec")
        _r.read_end("batchCreate_IN")

        tok = self._server.auth("batchCreate", "monitor.batch.create", {"rec":rec})
        self._simpl._set_rpc_token_data(tok)

        _x = self._simpl.batch_create(rec)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("batchCreate_OUT")
        _x._serialize(_w)
        _w.write_end("batchCreate_OUT")

        self._server.logger().debug("[batch_create - Done]")

        return _o

    def batch_read(self, _i):
        self._server.logger().debug("[batch_read - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("batchRead_IN")
        batch_id = _r.read_int64("batchId")
        batch_name = _r.read_string("batchName")
        _r.read_end("batchRead_IN")

        tok = self._server.auth("batchRead", "monitor.batch.read", {"batchId":batch_id, "batchName":batch_name})
        self._simpl._set_rpc_token_data(tok)

        _dvc = mettle.lib.DavCache()
        self._server_impl()._init_dav_cache("batchRead", _dvc)

        _dvc.clear()
        _dvc.add_targ("batch_name", mettle.lib.Dav(mettle.lib.Dav.eDavType.Max, 256))
        _dvc.validate("batch_name", batch_name)

        _x = self._simpl.batch_read(batch_id,
                                    batch_name)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("batchRead_OUT")
        _x._serialize(_w)
        _w.write_end("batchRead_OUT")

        self._server.logger().debug("[batch_read - Done]")

        return _o

    def batch_update(self, _i):
        self._server.logger().debug("[batch_update - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        rec = tBatch()

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("batchUpdate_IN")
        rec._deserialize(_r, "rec")
        _r.read_end("batchUpdate_IN")

        tok = self._server.auth("batchUpdate", "monitor.batch.update", {"rec":rec})
        self._simpl._set_rpc_token_data(tok)

        _x = self._simpl.batch_update(rec)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("batchUpdate_OUT")
        _x._serialize(_w)
        _w.write_end("batchUpdate_OUT")

        self._server.logger().debug("[batch_update - Done]")

        return _o

    def batch_delete(self, _i):
        self._server.logger().debug("[batch_delete - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("batchDelete_IN")
        batch_id = _r.read_int64("batchId")
        _r.read_end("batchDelete_IN")

        tok = self._server.auth("batchDelete", "monitor.batch.delete", {"batchId":batch_id})
        self._simpl._set_rpc_token_data(tok)

        self._simpl.batch_delete(batch_id)

        self._server.logger().debug("[batch_delete - Done]")

        return _o

    def batch_list(self, _i):
        self._server.logger().debug("[batch_list - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        lazy_load = bLazyLoad()
        query = bBatchQuery()

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("batchList_IN")
        lazy_load._deserialize(_r, "lazyLoad")
        query._deserialize(_r, "query")
        _r.read_end("batchList_IN")

        tok = self._server.auth("batchList", "monitor.batch.read", {"lazyLoad":lazy_load, "query":query})
        self._simpl._set_rpc_token_data(tok)

        _x = self._simpl.batch_list(lazy_load,
                                    query)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("batchList_OUT")
        _x._serialize(_w)
        _w.write_end("batchList_OUT")

        self._server.logger().debug("[batch_list - Done]")

        return _o

    def batch_lastest_inst_list(self, _i):
        self._server.logger().debug("[batch_lastest_inst_list - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        lazy_load = bLazyLoad()
        query = bBatchQuery()

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("batchLastestInstList_IN")
        lazy_load._deserialize(_r, "lazyLoad")
        query._deserialize(_r, "query")
        _r.read_end("batchLastestInstList_IN")

        tok = self._server.auth("batchLastestInstList", "monitor.batch.read", {"lazyLoad":lazy_load, "query":query})
        self._simpl._set_rpc_token_data(tok)

        _x = self._simpl.batch_lastest_inst_list(lazy_load,
                                                 query)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("batchLastestInstList_OUT")
        _x._serialize(_w)
        _w.write_end("batchLastestInstList_OUT")

        self._server.logger().debug("[batch_lastest_inst_list - Done]")

        return _o

    def batchinst_read(self, _i):
        self._server.logger().debug("[batchinst_read - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("batchinstRead_IN")
        batchinst_id = _r.read_int64("batchinstId")
        _r.read_end("batchinstRead_IN")

        tok = self._server.auth("batchinstRead", "monitor.batch.read", {"batchinstId":batchinst_id})
        self._simpl._set_rpc_token_data(tok)

        _x = self._simpl.batchinst_read(batchinst_id)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("batchinstRead_OUT")
        _x._serialize(_w)
        _w.write_end("batchinstRead_OUT")

        self._server.logger().debug("[batchinst_read - Done]")

        return _o

    def batchinst_list(self, _i):
        self._server.logger().debug("[batchinst_list - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        lazy_load = bLazyLoad()
        query = bBatchInstQuery()

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("batchinstList_IN")
        lazy_load._deserialize(_r, "lazyLoad")
        query._deserialize(_r, "query")
        _r.read_end("batchinstList_IN")

        tok = self._server.auth("batchinstList", "monitor.batch.read", {"lazyLoad":lazy_load, "query":query})
        self._simpl._set_rpc_token_data(tok)

        _x = self._simpl.batchinst_list(lazy_load,
                                        query)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("batchinstList_OUT")
        _x._serialize(_w)
        _w.write_end("batchinstList_OUT")

        self._server.logger().debug("[batchinst_list - Done]")

        return _o

    def batchitem_create(self, _i):
        self._server.logger().debug("[batchitem_create - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        rec = tBatchItem()

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("batchitemCreate_IN")
        rec._deserialize(_r, "rec")
        _r.read_end("batchitemCreate_IN")

        tok = self._server.auth("batchitemCreate", "monitor.batch.create", {"rec":rec})
        self._simpl._set_rpc_token_data(tok)

        _x = self._simpl.batchitem_create(rec)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("batchitemCreate_OUT")
        _x._serialize(_w)
        _w.write_end("batchitemCreate_OUT")

        self._server.logger().debug("[batchitem_create - Done]")

        return _o

    def batchitem_read(self, _i):
        self._server.logger().debug("[batchitem_read - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("batchitemRead_IN")
        batchitem_id = _r.read_int64("batchitemId")
        batch_id = _r.read_int64("batchId")
        _r.read_end("batchitemRead_IN")

        tok = self._server.auth("batchitemRead", "monitor.batch.read", {"batchitemId":batchitem_id, "batchId":batch_id})
        self._simpl._set_rpc_token_data(tok)

        _x = self._simpl.batchitem_read(batchitem_id,
                                        batch_id)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("batchitemRead_OUT")
        _x._serialize(_w)
        _w.write_end("batchitemRead_OUT")

        self._server.logger().debug("[batchitem_read - Done]")

        return _o

    def batchitem_update(self, _i):
        self._server.logger().debug("[batchitem_update - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        rec = tBatchItem()

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("batchitemUpdate_IN")
        rec._deserialize(_r, "rec")
        _r.read_end("batchitemUpdate_IN")

        tok = self._server.auth("batchitemUpdate", "monitor.batch.update", {"rec":rec})
        self._simpl._set_rpc_token_data(tok)

        _x = self._simpl.batchitem_update(rec)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("batchitemUpdate_OUT")
        _x._serialize(_w)
        _w.write_end("batchitemUpdate_OUT")

        self._server.logger().debug("[batchitem_update - Done]")

        return _o

    def batchitem_shift(self, _i):
        self._server.logger().debug("[batchitem_shift - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("batchitemShift_IN")
        batchitem_id = _r.read_int64("batchitemId")
        batch_id = _r.read_int64("batchId")
        shift_idx = _r.read_int64("shiftIdx")
        _r.read_end("batchitemShift_IN")

        tok = self._server.auth("batchitemShift", "monitor.batch.update", {"batchitemId":batchitem_id, "batchId":batch_id, "shiftIdx":shift_idx})
        self._simpl._set_rpc_token_data(tok)

        _x = self._simpl.batchitem_shift(batchitem_id,
                                         batch_id,
                                         shift_idx)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("batchitemShift_OUT")
        _x._serialize(_w)
        _w.write_end("batchitemShift_OUT")

        self._server.logger().debug("[batchitem_shift - Done]")

        return _o

    def batchitem_delete(self, _i):
        self._server.logger().debug("[batchitem_delete - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("batchitemDelete_IN")
        batchitem_id = _r.read_int64("batchitemId")
        batch_id = _r.read_int64("batchId")
        _r.read_end("batchitemDelete_IN")

        tok = self._server.auth("batchitemDelete", "monitor.batch.delete", {"batchitemId":batchitem_id, "batchId":batch_id})
        self._simpl._set_rpc_token_data(tok)

        self._simpl.batchitem_delete(batchitem_id,
                                     batch_id)

        self._server.logger().debug("[batchitem_delete - Done]")

        return _o

    def batchitem_list(self, _i):
        self._server.logger().debug("[batchitem_list - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("batchitemList_IN")
        batch_id = _r.read_int64("batchId")
        _r.read_end("batchitemList_IN")

        tok = self._server.auth("batchitemList", "monitor.batch.read", {"batchId":batch_id})
        self._simpl._set_rpc_token_data(tok)

        _x = self._simpl.batchitem_list(batch_id)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("batchitemList_OUT")
        _x._serialize(_w)
        _w.write_end("batchitemList_OUT")

        self._server.logger().debug("[batchitem_list - Done]")

        return _o

    def job_create(self, _i):
        self._server.logger().debug("[job_create - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        rec = tJob()

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("jobCreate_IN")
        rec._deserialize(_r, "rec")
        _r.read_end("jobCreate_IN")

        tok = self._server.auth("jobCreate", "monitor.job.create", {"rec":rec})
        self._simpl._set_rpc_token_data(tok)

        _x = self._simpl.job_create(rec)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("jobCreate_OUT")
        _x._serialize(_w)
        _w.write_end("jobCreate_OUT")

        self._server.logger().debug("[job_create - Done]")

        return _o

    def job_read(self, _i):
        self._server.logger().debug("[job_read - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("jobRead_IN")
        job_id = _r.read_int64("jobId")
        job_name = _r.read_string("jobName")
        _r.read_end("jobRead_IN")

        tok = self._server.auth("jobRead", "monitor.job.read", {"jobId":job_id, "jobName":job_name})
        self._simpl._set_rpc_token_data(tok)

        _dvc = mettle.lib.DavCache()
        self._server_impl()._init_dav_cache("jobRead", _dvc)

        _dvc.clear()
        _dvc.add_targ("job_name", mettle.lib.Dav(mettle.lib.Dav.eDavType.Max, 256))
        _dvc.validate("job_name", job_name)

        _x = self._simpl.job_read(job_id,
                                  job_name)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("jobRead_OUT")
        _x._serialize(_w)
        _w.write_end("jobRead_OUT")

        self._server.logger().debug("[job_read - Done]")

        return _o

    def job_update(self, _i):
        self._server.logger().debug("[job_update - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        rec = tJob()

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("jobUpdate_IN")
        rec._deserialize(_r, "rec")
        _r.read_end("jobUpdate_IN")

        tok = self._server.auth("jobUpdate", "monitor.job.update", {"rec":rec})
        self._simpl._set_rpc_token_data(tok)

        _x = self._simpl.job_update(rec)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("jobUpdate_OUT")
        _x._serialize(_w)
        _w.write_end("jobUpdate_OUT")

        self._server.logger().debug("[job_update - Done]")

        return _o

    def job_delete(self, _i):
        self._server.logger().debug("[job_delete - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("jobDelete_IN")
        job_id = _r.read_int64("jobId")
        _r.read_end("jobDelete_IN")

        tok = self._server.auth("jobDelete", "monitor.job.delete", {"jobId":job_id})
        self._simpl._set_rpc_token_data(tok)

        self._simpl.job_delete(job_id)

        self._server.logger().debug("[job_delete - Done]")

        return _o

    def job_list(self, _i):
        self._server.logger().debug("[job_list - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        lazy_load = bLazyLoad()

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("jobList_IN")
        lazy_load._deserialize(_r, "lazyLoad")
        wc_job_name = _r.read_string("wcJobName")
        wc_group = _r.read_string("wcGroup")
        _r.read_end("jobList_IN")

        tok = self._server.auth("jobList", "monitor.job.read", {"lazyLoad":lazy_load, "wcJobName":wc_job_name, "wcGroup":wc_group})
        self._simpl._set_rpc_token_data(tok)

        _dvc = mettle.lib.DavCache()
        self._server_impl()._init_dav_cache("jobList", _dvc)

        _dvc.clear()
        _dvc.add_targ("wc_job_name", mettle.lib.Dav(mettle.lib.Dav.eDavType.Max, 256))
        _dvc.validate("wc_job_name", wc_job_name)

        _dvc.clear()
        _dvc.add_targ("wc_group", mettle.lib.Dav(mettle.lib.Dav.eDavType.Max, 128))
        _dvc.validate("wc_group", wc_group)

        _x = self._simpl.job_list(lazy_load,
                                  wc_job_name,
                                  wc_group)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("jobList_OUT")
        _x._serialize(_w)
        _w.write_end("jobList_OUT")

        self._server.logger().debug("[job_list - Done]")

        return _o

    def job_schedule(self, _i):
        self._server.logger().debug("[job_schedule - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("jobSchedule_IN")
        job_id = _r.read_int64("jobId")
        job_name = _r.read_string("jobName")
        run_date = _r.read_datetime("runDate")
        extra_args = _r.read_string("extraArgs")
        priority = _r.read_int32("priority")
        parent_id = _r.read_int64("parentId")
        _r.read_end("jobSchedule_IN")

        tok = self._server.auth("jobSchedule", "monitor.job.run", {"jobId":job_id, "jobName":job_name, "runDate":run_date, "extraArgs":extra_args, "priority":priority, "parentId":parent_id})
        self._simpl._set_rpc_token_data(tok)

        _dvc = mettle.lib.DavCache()
        self._server_impl()._init_dav_cache("jobSchedule", _dvc)

        _dvc.clear()
        _dvc.add_targ("job_name", mettle.lib.Dav(mettle.lib.Dav.eDavType.Max, 256))
        _dvc.validate("job_name", job_name)

        _dvc.clear()
        _dvc.add_targ("extra_args", mettle.lib.Dav(mettle.lib.Dav.eDavType.Max, 256))
        _dvc.validate("extra_args", extra_args)

        _x = self._simpl.job_schedule(job_id,
                                      job_name,
                                      run_date,
                                      extra_args,
                                      priority,
                                      parent_id)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("jobSchedule_OUT")
        _x._serialize(_w)
        _w.write_end("jobSchedule_OUT")

        self._server.logger().debug("[job_schedule - Done]")

        return _o

    def jobinst_read(self, _i):
        self._server.logger().debug("[jobinst_read - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("jobinstRead_IN")
        jobinst_id = _r.read_int64("jobinstId")
        _r.read_end("jobinstRead_IN")

        tok = self._server.auth("jobinstRead", "monitor.jobinst.read", {"jobinstId":jobinst_id})
        self._simpl._set_rpc_token_data(tok)

        _x = self._simpl.jobinst_read(jobinst_id)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("jobinstRead_OUT")
        _x._serialize(_w)
        _w.write_end("jobinstRead_OUT")

        self._server.logger().debug("[jobinst_read - Done]")

        return _o

    def jobinst_rerun(self, _i):
        self._server.logger().debug("[jobinst_rerun - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("jobinstRerun_IN")
        jobinst_id = _r.read_int64("jobinstId")
        force_rerun = _r.read_bool("forceRerun")
        _r.read_end("jobinstRerun_IN")

        tok = self._server.auth("jobinstRerun", "monitor.jobinst.update", {"jobinstId":jobinst_id, "forceRerun":force_rerun})
        self._simpl._set_rpc_token_data(tok)

        _x = self._simpl.jobinst_rerun(jobinst_id,
                                       force_rerun)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("jobinstRerun_OUT")
        _x._serialize(_w)
        _w.write_end("jobinstRerun_OUT")

        self._server.logger().debug("[jobinst_rerun - Done]")

        return _o

    def jobinst_force_ok(self, _i):
        self._server.logger().debug("[jobinst_force_ok - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("jobinstForceOk_IN")
        jobinst_id = _r.read_int64("jobinstId")
        reason = _r.read_string("reason")
        _r.read_end("jobinstForceOk_IN")

        tok = self._server.auth("jobinstForceOk", "monitor.jobinst.update", {"jobinstId":jobinst_id, "reason":reason})
        self._simpl._set_rpc_token_data(tok)

        _dvc = mettle.lib.DavCache()
        self._server_impl()._init_dav_cache("jobinstForceOk", _dvc)

        _dvc.clear()
        _dvc.add_targ("reason", mettle.lib.Dav(mettle.lib.Dav.eDavType.Max, 256))
        _dvc.validate("reason", reason)

        _x = self._simpl.jobinst_force_ok(jobinst_id,
                                          reason)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("jobinstForceOk_OUT")
        _x._serialize(_w)
        _w.write_end("jobinstForceOk_OUT")

        self._server.logger().debug("[jobinst_force_ok - Done]")

        return _o

    def jobinst_stop(self, _i):
        self._server.logger().debug("[jobinst_stop - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("jobinstStop_IN")
        jobinst_id = _r.read_int64("jobinstId")
        reason = _r.read_string("reason")
        _r.read_end("jobinstStop_IN")

        tok = self._server.auth("jobinstStop", "monitor.jobinst.update", {"jobinstId":jobinst_id, "reason":reason})
        self._simpl._set_rpc_token_data(tok)

        _dvc = mettle.lib.DavCache()
        self._server_impl()._init_dav_cache("jobinstStop", _dvc)

        _dvc.clear()
        _dvc.add_targ("reason", mettle.lib.Dav(mettle.lib.Dav.eDavType.Max, 256))
        _dvc.validate("reason", reason)

        self._simpl.jobinst_stop(jobinst_id,
                                 reason)

        self._server.logger().debug("[jobinst_stop - Done]")

        return _o

    def jobinst_list(self, _i):
        self._server.logger().debug("[jobinst_list - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        lazy_load = bLazyLoad()
        query = bJobInstQuery()

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("jobinstList_IN")
        lazy_load._deserialize(_r, "lazyLoad")
        query._deserialize(_r, "query")
        _r.read_end("jobinstList_IN")

        tok = self._server.auth("jobinstList", "monitor.jobinst.read", {"lazyLoad":lazy_load, "query":query})
        self._simpl._set_rpc_token_data(tok)

        _x = self._simpl.jobinst_list(lazy_load,
                                      query)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("jobinstList_OUT")
        _x._serialize(_w)
        _w.write_end("jobinstList_OUT")

        self._server.logger().debug("[jobinst_list - Done]")

        return _o

    def jobinst_metrics(self, _i):
        self._server.logger().debug("[jobinst_metrics - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        lazy_load = bLazyLoad()

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("jobinstMetrics_IN")
        lazy_load._deserialize(_r, "lazyLoad")
        jobinst_id = _r.read_int64("jobinstId")
        _r.read_end("jobinstMetrics_IN")

        tok = self._server.auth("jobinstMetrics", "monitor.jobinst.read", {"lazyLoad":lazy_load, "jobinstId":jobinst_id})
        self._simpl._set_rpc_token_data(tok)

        _x = self._simpl.jobinst_metrics(lazy_load,
                                         jobinst_id)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("jobinstMetrics_OUT")
        _x._serialize(_w)
        _w.write_end("jobinstMetrics_OUT")

        self._server.logger().debug("[jobinst_metrics - Done]")

        return _o

    def mondate_create(self, _i):
        self._server.logger().debug("[mondate_create - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        rec = tMondate()

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("mondateCreate_IN")
        rec._deserialize(_r, "rec")
        _r.read_end("mondateCreate_IN")

        tok = self._server.auth("mondateCreate", "monitor.mondate.create", {"rec":rec})
        self._simpl._set_rpc_token_data(tok)

        _x = self._simpl.mondate_create(rec)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("mondateCreate_OUT")
        _x._serialize(_w)
        _w.write_end("mondateCreate_OUT")

        self._server.logger().debug("[mondate_create - Done]")

        return _o

    def mondate_read(self, _i):
        self._server.logger().debug("[mondate_read - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("mondateRead_IN")
        mondate_id = _r.read_string("mondateId")
        _r.read_end("mondateRead_IN")

        tok = self._server.auth("mondateRead", "monitor.mondate.read", {"mondateId":mondate_id})
        self._simpl._set_rpc_token_data(tok)

        _dvc = mettle.lib.DavCache()
        self._server_impl()._init_dav_cache("mondateRead", _dvc)

        _dvc.clear()
        _dvc.add_targ("mondate_id", mettle.lib.Dav(mettle.lib.Dav.eDavType.Max, 16))
        _dvc.validate("mondate_id", mondate_id)

        _x = self._simpl.mondate_read(mondate_id)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("mondateRead_OUT")
        _x._serialize(_w)
        _w.write_end("mondateRead_OUT")

        self._server.logger().debug("[mondate_read - Done]")

        return _o

    def mondate_update(self, _i):
        self._server.logger().debug("[mondate_update - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        rec = tMondate()

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("mondateUpdate_IN")
        rec._deserialize(_r, "rec")
        _r.read_end("mondateUpdate_IN")

        tok = self._server.auth("mondateUpdate", "monitor.mondate.update", {"rec":rec})
        self._simpl._set_rpc_token_data(tok)

        _x = self._simpl.mondate_update(rec)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("mondateUpdate_OUT")
        _x._serialize(_w)
        _w.write_end("mondateUpdate_OUT")

        self._server.logger().debug("[mondate_update - Done]")

        return _o

    def mondate_delete(self, _i):
        self._server.logger().debug("[mondate_delete - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("mondateDelete_IN")
        mondate_id = _r.read_string("mondateId")
        _r.read_end("mondateDelete_IN")

        tok = self._server.auth("mondateDelete", "monitor.mondate.delete", {"mondateId":mondate_id})
        self._simpl._set_rpc_token_data(tok)

        _dvc = mettle.lib.DavCache()
        self._server_impl()._init_dav_cache("mondateDelete", _dvc)

        _dvc.clear()
        _dvc.add_targ("mondate_id", mettle.lib.Dav(mettle.lib.Dav.eDavType.Max, 16))
        _dvc.validate("mondate_id", mondate_id)

        self._simpl.mondate_delete(mondate_id)

        self._server.logger().debug("[mondate_delete - Done]")

        return _o

    def mondate_list(self, _i):
        self._server.logger().debug("[mondate_list - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        lazy_load = bLazyLoad()

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("mondateList_IN")
        lazy_load._deserialize(_r, "lazyLoad")
        wc_mondate_id = _r.read_string("wcMondateId")
        _r.read_end("mondateList_IN")

        tok = self._server.auth("mondateList", "monitor.mondate.read", {"lazyLoad":lazy_load, "wcMondateId":wc_mondate_id})
        self._simpl._set_rpc_token_data(tok)

        _dvc = mettle.lib.DavCache()
        self._server_impl()._init_dav_cache("mondateList", _dvc)

        _dvc.clear()
        _dvc.add_targ("wc_mondate_id", mettle.lib.Dav(mettle.lib.Dav.eDavType.Max, 16))
        _dvc.validate("wc_mondate_id", wc_mondate_id)

        _x = self._simpl.mondate_list(lazy_load,
                                      wc_mondate_id)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("mondateList_OUT")
        _x._serialize(_w)
        _w.write_end("mondateList_OUT")

        self._server.logger().debug("[mondate_list - Done]")

        return _o

    def mondate_increment(self, _i):
        self._server.logger().debug("[mondate_increment - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("mondateIncrement_IN")
        mondate_id = _r.read_string("mondateId")
        days_value = _r.read_int64("daysValue")
        _r.read_end("mondateIncrement_IN")

        tok = self._server.auth("mondateIncrement", "monitor.mondate.update", {"mondateId":mondate_id, "daysValue":days_value})
        self._simpl._set_rpc_token_data(tok)

        _dvc = mettle.lib.DavCache()
        self._server_impl()._init_dav_cache("mondateIncrement", _dvc)

        _dvc.clear()
        _dvc.add_targ("mondate_id", mettle.lib.Dav(mettle.lib.Dav.eDavType.Max, 16))
        _dvc.validate("mondate_id", mondate_id)

        _x = self._simpl.mondate_increment(mondate_id,
                                           days_value)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("mondateIncrement_OUT")
        _x._serialize(_w)
        _w.write_end("mondateIncrement_OUT")

        self._server.logger().debug("[mondate_increment - Done]")

        return _o

    def mondate_set_value(self, _i):
        self._server.logger().debug("[mondate_set_value - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None
        _w = None
        _x = None

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("mondateSetValue_IN")
        mondate_id = _r.read_string("mondateId")
        value = _r.read_date("value")
        _r.read_end("mondateSetValue_IN")

        tok = self._server.auth("mondateSetValue", "monitor.mondate.update", {"mondateId":mondate_id, "value":value})
        self._simpl._set_rpc_token_data(tok)

        _dvc = mettle.lib.DavCache()
        self._server_impl()._init_dav_cache("mondateSetValue", _dvc)

        _dvc.clear()
        _dvc.add_targ("mondate_id", mettle.lib.Dav(mettle.lib.Dav.eDavType.Max, 16))
        _dvc.validate("mondate_id", mondate_id)

        _x = self._simpl.mondate_set_value(mondate_id,
                                           value)

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("mondateSetValue_OUT")
        _x._serialize(_w)
        _w.write_end("mondateSetValue_OUT")

        self._server.logger().debug("[mondate_set_value - Done]")

        return _o

    def purge_history(self, _i):
        self._server.logger().debug("[purge_history - Start]")

        _o = self._server.get_transport().new_stream()
        _r = None

        _r = self._server.get_transport().new_reader(_i)
        _r.read_start("purgeHistory_IN")
        from_date = _r.read_date("fromDate")
        _r.read_end("purgeHistory_IN")

        tok = self._server.auth("purgeHistory", "monitor.purge", {"fromDate":from_date})
        self._simpl._set_rpc_token_data(tok)

        self._simpl.purge_history(from_date)

        self._server.logger().debug("[purge_history - Done]")

        return _o

    def server_date_time(self, _i):
        self._server.logger().debug("[server_date_time - Start]")

        _o = self._server.get_transport().new_stream()
        _w = None
        _x = None

        tok = self._server.auth("serverDateTime", "__LOGGED_IN__", {})
        self._simpl._set_rpc_token_data(tok)

        _x = self._simpl.server_date_time()

        _w = self._server.get_transport().new_writer(_o)
        _w.write_start("serverDateTime_OUT")
        _w.write_datetime("return", _x)
        _w.write_end("serverDateTime_OUT")

        self._server.logger().debug("[server_date_time - Done]")

        return _o

